{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import textstat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Data Cleaning: Perform standard text preprocessing tasks, including: Removing stop words, punctuation, and special\n",
    "characters, Lowercasing the text, Tokenizing the reviews, Stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment to numeric values: 1 for positive, 0 for negative\n",
    "df['sentiment_numeric'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DEFINED IN TASK 8\n",
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m107/196\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7:06\u001b[0m 5s/step - accuracy: 0.4927 - loss: 0.6942"
     ]
    }
   ],
   "source": [
    "# LSTM deneme for BoW\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=3000)\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment_numeric'], test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "#lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=128)\n",
    "\n",
    "print(\"LSTM with BoW:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW PART 1\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=3000)\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Bigram\n",
    "vectorizer_bow_bigram = CountVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Trigram\n",
    "vectorizer_bow_trigram = CountVectorizer(max_features=3000, ngram_range=(1, 3))\n",
    "X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Özellik şekillerini kontrol etme\n",
    "print(\"BoW Unigram Feature Shape:\", X_bow_unigram.shape)\n",
    "print(\"BoW Bigram Feature Shape:\", X_bow_bigram.shape)\n",
    "print(\"BoW Trigram Feature Shape:\", X_bow_trigram.shape)\n",
    "\n",
    "\n",
    "# For UniGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For BiGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_bigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For TriGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_trigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW PART 2\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=5000)\n",
    "#X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review']).toarray()\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review'])  # toarray() olmadan\n",
    "\n",
    "\n",
    "# Bigram\n",
    "vectorizer_bow_bigram = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "#X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review']).toarray()\n",
    "X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Trigram\n",
    "vectorizer_bow_trigram = CountVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "#X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review']).toarray()\n",
    "X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Özellik şekillerini kontrol etme\n",
    "print(\"BoW Unigram Feature Shape:\", X_bow_unigram.shape)\n",
    "print(\"BoW Bigram Feature Shape:\", X_bow_bigram.shape)\n",
    "print(\"BoW Trigram Feature Shape:\", X_bow_trigram.shape)\n",
    "\n",
    "\n",
    "# For UniGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For BiGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_bigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For TriGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_trigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BOW PART 3\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=7000)\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Bigram\n",
    "vectorizer_bow_bigram = CountVectorizer(max_features=7000, ngram_range=(1, 2))\n",
    "X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Trigram\n",
    "vectorizer_bow_trigram = CountVectorizer(max_features=7000, ngram_range=(1, 3))\n",
    "X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Özellik şekillerini kontrol etme\n",
    "print(\"BoW Unigram Feature Shape:\", X_bow_unigram.shape)\n",
    "print(\"BoW Bigram Feature Shape:\", X_bow_bigram.shape)\n",
    "print(\"BoW Trigram Feature Shape:\", X_bow_trigram.shape)\n",
    "\n",
    "\n",
    "# For UniGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For BiGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_bigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For TriGram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_trigram, df['sentiment'], test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
