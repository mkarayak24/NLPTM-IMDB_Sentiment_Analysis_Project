{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import textstat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Data Cleaning: Perform standard text preprocessing tasks, including: Removing stop words, punctuation, and special\n",
    "characters, Lowercasing the text, Tokenizing the reviews, Stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DEFINED IN TASK 7\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all reviews\n",
    "df['tokens'] = df['cleaned_review'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DEFINED IN TASK 8\n",
    "\n",
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape with window size 3: (50000, 100)\n",
      "Word2Vec Feature Shape with window size 5: (50000, 100)\n",
      "Word2Vec Feature Shape with window size 7: (50000, 100)\n",
      "Logistic Regression model with word2vec with window size 3\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.87\n",
      "F1-Score: 0.87\n",
      "Logistic Regression model with word2vec with window size 5\n",
      "Accuracy: 0.87\n",
      "Precision: 0.87\n",
      "Recall: 0.88\n",
      "F1-Score: 0.87\n",
      "Logistic Regression model with word2vec with window size 7\n",
      "Accuracy: 0.87\n",
      "Precision: 0.87\n",
      "Recall: 0.88\n",
      "F1-Score: 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"\\n# Train Word2Vec model\\nword2vec_model = Word2Vec(sentences=df[\\'tokens\\'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\\nX_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in df[\\'tokens\\']])\\n\\n\\n# Check Word2Vec features\\nprint(\"Word2Vec Feature Shape:\", X_word2vec.shape)\\n\\n# Split data into training and testing\\nX_train, X_test, y_train, y_test = train_test_split(X_word2vec, df[\\'sentiment\\'], test_size=0.2, random_state=42)\\n\\n# Train a Logistic Regression model\\nclf_word2vec = LogisticRegression(max_iter=1000)\\nclf_word2vec.fit(X_train, y_train)\\nprint(\"Logistic Regression model with word2vec\")\\nevaluate_model(y_test, clf_word2vec.predict(X_test))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WORD2VEC PART 1 - TUNING WINDOWS SIZE\n",
    "\n",
    "# Pencere boyutu 3\n",
    "word2vec_model_window_3 = Word2Vec(sentences=df['tokens'], vector_size=100, window=3, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_window_3 = np.array([np.mean([word2vec_model_window_3.wv[word] for word in words if word in word2vec_model_window_3.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with window size 3:\", X_word2vec_window_3.shape)\n",
    "\n",
    "# Pencere boyutu 5 (varsayÄ±lan)\n",
    "word2vec_model_window_5 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_window_5 = np.array([np.mean([word2vec_model_window_5.wv[word] for word in words if word in word2vec_model_window_5.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with window size 5:\", X_word2vec_window_5.shape)\n",
    "\n",
    "# Pencere boyutu 7\n",
    "word2vec_model_window_7 = Word2Vec(sentences=df['tokens'], vector_size=100, window=7, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_window_7 = np.array([np.mean([word2vec_model_window_7.wv[word] for word in words if word in word2vec_model_window_7.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with window size 7:\", X_word2vec_window_7.shape)\n",
    "\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_window_3, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with window size 3\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_window_5, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with window size 5\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_window_7, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with window size 7\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape with embedding dimension 50: (50000, 50)\n",
      "Word2Vec Feature Shape with embedding dimension 100: (50000, 100)\n",
      "Word2Vec Feature Shape with embedding dimension 200: (50000, 200)\n",
      "Logistic Regression model with word2vec with embedding dimension 50\n",
      "Accuracy: 0.85\n",
      "Precision: 0.85\n",
      "Recall: 0.86\n",
      "F1-Score: 0.86\n",
      "Logistic Regression model with word2vec with embedding dimension 100\n",
      "Accuracy: 0.87\n",
      "Precision: 0.86\n",
      "Recall: 0.88\n",
      "F1-Score: 0.87\n",
      "Logistic Regression model with word2vec with embedding dimension 200\n",
      "Accuracy: 0.87\n",
      "Precision: 0.87\n",
      "Recall: 0.88\n",
      "F1-Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "#WORD2VEC PART 2 - TUNING EMBEDDING DIMENTSION\n",
    "\n",
    "# GÃ¶mme boyutu 50\n",
    "word2vec_model_dim_50 = Word2Vec(sentences=df['tokens'], vector_size=50, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_dim_50 = np.array([np.mean([word2vec_model_dim_50.wv[word] for word in words if word in word2vec_model_dim_50.wv] or [np.zeros(50)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with embedding dimension 50:\", X_word2vec_dim_50.shape)\n",
    "\n",
    "# GÃ¶mme boyutu 100 (varsayÄ±lan)\n",
    "word2vec_model_dim_100 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_dim_100 = np.array([np.mean([word2vec_model_dim_100.wv[word] for word in words if word in word2vec_model_dim_100.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with embedding dimension 100:\", X_word2vec_dim_100.shape)\n",
    "\n",
    "# GÃ¶mme boyutu 200\n",
    "word2vec_model_dim_200 = Word2Vec(sentences=df['tokens'], vector_size=200, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_dim_200 = np.array([np.mean([word2vec_model_dim_200.wv[word] for word in words if word in word2vec_model_dim_200.wv] or [np.zeros(200)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with embedding dimension 200:\", X_word2vec_dim_200.shape)\n",
    "\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_dim_50, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with embedding dimension 50\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_dim_100, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with embedding dimension 100\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_dim_200, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with embedding dimension 200\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape with 5 epochs: (50000, 100)\n",
      "Word2Vec Feature Shape with 10 epochs: (50000, 100)\n",
      "Word2Vec Feature Shape with 20 epochs: (50000, 100)\n",
      "Logistic Regression model with word2vec with 5 epochs\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.87\n",
      "F1-Score: 0.86\n",
      "Logistic Regression model with word2vec with 10 epochs\n",
      "Accuracy: 0.87\n",
      "Precision: 0.87\n",
      "Recall: 0.87\n",
      "F1-Score: 0.87\n",
      "Logistic Regression model with word2vec with 20 epochs\n",
      "Accuracy: 0.87\n",
      "Precision: 0.87\n",
      "Recall: 0.88\n",
      "F1-Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "#WORD2VEC PART 3 - TUNING TRAINING EPOCHS\n",
    "\n",
    "# EÄitim epoch'larÄ± 5\n",
    "word2vec_model_epochs_5 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=5)\n",
    "X_word2vec_epochs_5 = np.array([np.mean([word2vec_model_epochs_5.wv[word] for word in words if word in word2vec_model_epochs_5.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with 5 epochs:\", X_word2vec_epochs_5.shape)\n",
    "\n",
    "# EÄitim epoch'larÄ± 10 (varsayÄ±lan)\n",
    "word2vec_model_epochs_10 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_epochs_10 = np.array([np.mean([word2vec_model_epochs_10.wv[word] for word in words if word in word2vec_model_epochs_10.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with 10 epochs:\", X_word2vec_epochs_10.shape)\n",
    "\n",
    "# EÄitim epoch'larÄ± 20\n",
    "word2vec_model_epochs_20 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=20)\n",
    "X_word2vec_epochs_20 = np.array([np.mean([word2vec_model_epochs_20.wv[word] for word in words if word in word2vec_model_epochs_20.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with 20 epochs:\", X_word2vec_epochs_20.shape)\n",
    "\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_epochs_5, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with 5 epochs\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_epochs_10, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with 10 epochs\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_epochs_20, df['sentiment'], test_size=0.2, random_state=42)\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with 20 epochs\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
