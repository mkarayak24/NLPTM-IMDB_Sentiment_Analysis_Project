{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import textstat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Data Cleaning: Perform standard text preprocessing tasks, including: Removing stop words, punctuation, and special\n",
    "characters, Lowercasing the text, Tokenizing the reviews, Stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DEFINED IN TASK 7\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all reviews\n",
    "df['tokens'] = df['cleaned_review'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DEFINED IN TASK 8\n",
    "\n",
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape with window size 3: (50000, 100)\n",
      "Word2Vec Feature Shape with window size 5: (50000, 100)\n",
      "Word2Vec Feature Shape with window size 7: (50000, 100)\n",
      "Logistic Regression model with word2vec with window size 3\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.87\n",
      "F1-Score: 0.87\n",
      "SVM with word2vec with window size 3:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.87\n",
      "F1-Score: 0.87\n",
      "Random Forest with word2vec with window size 3:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.83\n",
      "Recall: 0.85\n",
      "F1-Score: 0.84\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m evaluate_model(y_test, rf_word2vec\u001b[38;5;241m.\u001b[39mpredict(X_test))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# LSTM model:\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m lstm_model_word2vec \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m     39\u001b[0m lstm_model_word2vec\u001b[38;5;241m.\u001b[39madd(Embedding(input_dim\u001b[38;5;241m=\u001b[39mX_word2vec_window_3\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, input_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)) \n\u001b[0;32m     40\u001b[0m lstm_model_word2vec\u001b[38;5;241m.\u001b[39madd(LSTM(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, recurrent_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "#WORD2VEC PART 1 - TUNING WINDOWS SIZE\n",
    "\n",
    "# Pencere boyutu 3\n",
    "word2vec_model_window_3 = Word2Vec(sentences=df['tokens'], vector_size=100, window=3, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_window_3 = np.array([np.mean([word2vec_model_window_3.wv[word] for word in words if word in word2vec_model_window_3.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with window size 3:\", X_word2vec_window_3.shape)\n",
    "\n",
    "# Pencere boyutu 5 (varsayılan)\n",
    "word2vec_model_window_5 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_window_5 = np.array([np.mean([word2vec_model_window_5.wv[word] for word in words if word in word2vec_model_window_5.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with window size 5:\", X_word2vec_window_5.shape)\n",
    "\n",
    "# Pencere boyutu 7\n",
    "word2vec_model_window_7 = Word2Vec(sentences=df['tokens'], vector_size=100, window=7, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_window_7 = np.array([np.mean([word2vec_model_window_7.wv[word] for word in words if word in word2vec_model_window_7.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with window size 7:\", X_word2vec_window_7.shape)\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_window_3, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with window size 3\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with window size 3:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with window size 3:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_window_3.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with window size 3:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_window_5, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with window size 5\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with window size 5:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with window size 5:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_window_5.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with window size 5:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_window_7, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with window size 7\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with window size 7:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with window size 7:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_window_7.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with window size 7:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD2VEC PART 2 - TUNING EMBEDDING DIMENTSION\n",
    "\n",
    "# Gömme boyutu 50\n",
    "word2vec_model_dim_50 = Word2Vec(sentences=df['tokens'], vector_size=50, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_dim_50 = np.array([np.mean([word2vec_model_dim_50.wv[word] for word in words if word in word2vec_model_dim_50.wv] or [np.zeros(50)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with embedding dimension 50:\", X_word2vec_dim_50.shape)\n",
    "\n",
    "# Gömme boyutu 100 (varsayılan)\n",
    "word2vec_model_dim_100 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_dim_100 = np.array([np.mean([word2vec_model_dim_100.wv[word] for word in words if word in word2vec_model_dim_100.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with embedding dimension 100:\", X_word2vec_dim_100.shape)\n",
    "\n",
    "# Gömme boyutu 200\n",
    "word2vec_model_dim_200 = Word2Vec(sentences=df['tokens'], vector_size=200, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_dim_200 = np.array([np.mean([word2vec_model_dim_200.wv[word] for word in words if word in word2vec_model_dim_200.wv] or [np.zeros(200)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with embedding dimension 200:\", X_word2vec_dim_200.shape)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_dim_50, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with embedding dimension 50\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with embedding dimension 50:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with embedding dimension 50:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_dim_50.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with embedding dimension 50:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_dim_100, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with embedding dimension 100\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with embedding dimension 100:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with embedding dimension 100:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_dim_100.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with embedding dimension 100:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_dim_200, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with embedding dimension 200\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with embedding dimension 200:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with embedding dimension 102000:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_dim_200.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with embedding dimension 200:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD2VEC PART 3 - TUNING TRAINING EPOCHS\n",
    "\n",
    "# Eğitim epoch'ları 5\n",
    "word2vec_model_epochs_5 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=5)\n",
    "X_word2vec_epochs_5 = np.array([np.mean([word2vec_model_epochs_5.wv[word] for word in words if word in word2vec_model_epochs_5.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with 5 epochs:\", X_word2vec_epochs_5.shape)\n",
    "\n",
    "# Eğitim epoch'ları 10 (varsayılan)\n",
    "word2vec_model_epochs_10 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec_epochs_10 = np.array([np.mean([word2vec_model_epochs_10.wv[word] for word in words if word in word2vec_model_epochs_10.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with 10 epochs:\", X_word2vec_epochs_10.shape)\n",
    "\n",
    "# Eğitim epoch'ları 20\n",
    "word2vec_model_epochs_20 = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=20)\n",
    "X_word2vec_epochs_20 = np.array([np.mean([word2vec_model_epochs_20.wv[word] for word in words if word in word2vec_model_epochs_20.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "print(\"Word2Vec Feature Shape with 20 epochs:\", X_word2vec_epochs_20.shape)\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_epochs_5, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with 5 epochs\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with 5 epochs:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with 5 epochs:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_epochs_5.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with 5 epochs:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_epochs_10, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with 10 epochs\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with 10 epochs:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with 10 epochs:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_epochs_10.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with 10 epochs:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec_epochs_20, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec with 20 epochs\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec with 20 epochs:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec with 20 epochs:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=X_word2vec_epochs_20.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with word2vec with 20 epochs:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
