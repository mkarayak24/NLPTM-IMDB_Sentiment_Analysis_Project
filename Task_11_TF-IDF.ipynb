{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Keaton\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import textstat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Data Cleaning: Perform standard text preprocessing tasks, including: Removing stop words, punctuation, and special\n",
    "characters, Lowercasing the text, Tokenizing the reviews, Stemming or lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALREADY DEFINED IN TASK 8\n",
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Unigram Feature Shape: (50000, 3000)\n",
      "TF-IDF Bigram Feature Shape: (50000, 3000)\n",
      "TF-IDF Trigram Feature Shape: (50000, 3000)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF PART 1\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_tfidf_unigram_3000_feat = TfidfVectorizer(max_features=3000)\n",
    "X_tfidf_unigram_3000_feat = vectorizer_tfidf_unigram_3000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Bigram\n",
    "vectorizer_tfidf_bigram_3000_feat = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_tfidf_bigram_3000_feat = vectorizer_tfidf_bigram_3000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Trigram\n",
    "vectorizer_tfidf_trigram_3000_feat = TfidfVectorizer(max_features=3000, ngram_range=(1, 3))\n",
    "X_tfidf_trigram_3000_feat = vectorizer_tfidf_trigram_3000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Özellik şekillerini kontrol etme\n",
    "print(\"TF-IDF Unigram Feature Shape:\", X_tfidf_unigram_3000_feat.shape)\n",
    "print(\"TF-IDF Bigram Feature Shape:\", X_tfidf_bigram_3000_feat.shape)\n",
    "print(\"TF-IDF Trigram Feature Shape:\", X_tfidf_trigram_3000_feat.shape)\n",
    "\n",
    "\n",
    "# For Unigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_unigram_3000_feat, df['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Keaton\\anaconda3\\envs\\nlp-project\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 95ms/step - accuracy: 0.7769 - loss: 0.4672\n",
      "Epoch 2/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 115ms/step - accuracy: 0.8737 - loss: 0.3083\n",
      "Epoch 3/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 105ms/step - accuracy: 0.8902 - loss: 0.2713\n",
      "Epoch 4/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 108ms/step - accuracy: 0.9065 - loss: 0.2358\n",
      "Epoch 5/5\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 112ms/step - accuracy: 0.9169 - loss: 0.2143\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step\n",
      "LSTM with integer-encoded text sequences:\n",
      "Accuracy: 0.8726\n",
      "Precision: 0.8881\n",
      "Recall: 0.8549\n",
      "F1 Score: 0.8712\n"
     ]
    }
   ],
   "source": [
    "# LSTM deneme\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model_lstm(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label=1)\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label=1)\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label=1)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=3000)  # Set a limit of 3000 most common words\n",
    "tokenizer.fit_on_texts(df['cleaned_review'])\n",
    "\n",
    "# Convert text to sequences\n",
    "X_sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
    "X_padded = pad_sequences(X_sequences, maxlen=100)  # Padding sequences to a max length of 100\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Label encode the target variable (sentiment)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # \"negative\" -> 0, \"positive\" -> 1\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(input_dim=3000, output_dim=100, input_length=100))  # Embedding layer\n",
    "lstm_model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))          # LSTM layer\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))                               # Output layer for binary classification\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "lstm_model.fit(X_train, y_train_encoded, epochs=5, batch_size=32)\n",
    "\n",
    "# Tahminleri ikili değerlere (0 veya 1) dönüştürme\n",
    "predicted_labels = (lstm_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"LSTM with integer-encoded text sequences:\")\n",
    "evaluate_model_lstm(y_test_encoded, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with TF-IDF using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For Bigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_bigram_3000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with TF-IDF using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For Trigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_trigram_3000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with TF-IDF using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF PART 2\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_tfidf_unigram_5000_feat = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf_unigram_5000_feat = vectorizer_tfidf_unigram_5000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Bigram\n",
    "vectorizer_tfidf_bigram_5000_feat = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_tfidf_bigram_5000_feat = vectorizer_tfidf_bigram_5000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Trigram\n",
    "vectorizer_tfidf_trigram_5000_feat = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "X_tfidf_trigram_5000_feat = vectorizer_tfidf_trigram_5000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Özellik şekillerini kontrol etme\n",
    "print(\"TF-IDF Unigram Feature Shape:\", X_tfidf_unigram_5000_feat.shape)\n",
    "print(\"TF-IDF Bigram Feature Shape:\", X_tfidf_bigram_5000_feat.shape)\n",
    "print(\"TF-IDF Trigram Feature Shape:\", X_tfidf_trigram_5000_feat.shape)\n",
    "\n",
    "\n",
    "\n",
    "# For Unigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_unigram_5000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with TF-IDF using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For Bigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_bigram_5000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with TF-IDF using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For Trigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_trigram_5000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with TF-IDF using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF PART 3\n",
    "\n",
    "# Unigram (varsayılan)\n",
    "vectorizer_tfidf_unigram_7000_feat = TfidfVectorizer(max_features=7000)\n",
    "X_tfidf_unigram_7000_feat = vectorizer_tfidf_unigram_7000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Bigram\n",
    "vectorizer_tfidf_bigram_7000_feat = TfidfVectorizer(max_features=7000, ngram_range=(1, 2))\n",
    "X_tfidf_bigram_7000_feat = vectorizer_tfidf_bigram_7000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Trigram\n",
    "vectorizer_tfidf_trigram_7000_feat = TfidfVectorizer(max_features=7000, ngram_range=(1, 3))\n",
    "X_tfidf_trigram_7000_feat = vectorizer_tfidf_trigram_7000_feat.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Özellik şekillerini kontrol etme\n",
    "print(\"TF-IDF Unigram Feature Shape:\", X_tfidf_unigram_7000_feat.shape)\n",
    "print(\"TF-IDF Bigram Feature Shape:\", X_tfidf_bigram_7000_feat.shape)\n",
    "print(\"TF-IDF Trigram Feature Shape:\", X_tfidf_trigram_7000_feat.shape)\n",
    "\n",
    "# For Unigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_unigram_7000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with TF-IDF using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For Bigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_bigram_7000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with TF-IDF using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n",
    "\n",
    "# For Trigram\n",
    "#######################################################################################################################################\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_trigram_7000_feat, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with TF-IDF using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n",
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))\n",
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))\n",
    "\n",
    "\"\"\"\"\n",
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "print(\"LSTM with TF-IDF using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))\n",
    "\"\"\"\n",
    "#######################################################################################################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
