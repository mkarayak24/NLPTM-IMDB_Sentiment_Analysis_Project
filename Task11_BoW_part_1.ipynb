{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning NLP Feature Extraction Techniques\n",
    " \n",
    "11- In this task, you will focus on optimizing the NLP feature extraction\n",
    "methods used in the sentiment classification models. Begin by exploring different configurations of the BoW, TF-IDF\n",
    "techniques, such as adjusting the n-gram range, using unigrams, bigrams, or trigrams, and varying the maximum number of\n",
    "features to include in the vocabulary. For word embeddings, experiment with fine-tuning Word2Vec, GloVe, or BERT by\n",
    "adjusting parameters like the window size, embedding dimension, and training epochs for Word2Vec, or by applying BERT\n",
    "fine-tuning with different learning rates and batch sizes. Once you havve fine-tuned these feature extraction techniques, apply\n",
    "them to the training dataset and evaluate how these changes impact the model's performance on the test dataset. You are\n",
    "required to compare the optimized feature extraction methods with the baseline configurations used in Task 10. Evaluate the\n",
    "results using metrics such as accuracy, precision, recall, and F1-score, and provide insights into how adjusting the NLP-based\n",
    "features influences the overall sentiment prediction performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset_Preprocessed.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data size\n",
    "print(\"Dataset Size:\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3000 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=3000)\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW Unigram Feature Shape with 3000 features:\", X_bow_unigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using unigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram\n",
    "vectorizer_bow_bigram = CountVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW Bigram Feature Shape with 3000 features:\", X_bow_bigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_bigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using bigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram\n",
    "vectorizer_bow_trigram = CountVectorizer(max_features=3000, ngram_range=(1, 3))\n",
    "X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW Trigram Feature Shape with 3000 features:\", X_bow_trigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_trigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using trigrams with 3000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5000 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=5000)\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW Unigram Feature Shape with 5000 features:\", X_bow_unigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using unigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow_bigram = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW bigram Feature Shape with 5000 features:\", X_bow_bigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_bigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using bigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram\n",
    "vectorizer_bow_trigram = CountVectorizer(max_features=5000, ngram_range=(1, 3))\n",
    "X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW trigram Feature Shape with 5000 features:\", X_bow_trigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_trigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using trigrams with 5000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7000 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram (varsayılan)\n",
    "vectorizer_bow_unigram = CountVectorizer(max_features=7000)\n",
    "X_bow_unigram = vectorizer_bow_unigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW unigram Feature Shape with 7000 features:\", X_bow_unigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_unigram, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using unigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram\n",
    "vectorizer_bow_bigram = CountVectorizer(max_features=7000, ngram_range=(1, 2))\n",
    "X_bow_bigram = vectorizer_bow_bigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW bigram Feature Shape with 7000 features:\", X_bow_bigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_bigram, df['sentiment_numeric'], test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using bigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigram\n",
    "vectorizer_bow_trigram = CountVectorizer(max_features=7000, ngram_range=(1, 3))\n",
    "X_bow_trigram = vectorizer_bow_trigram.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "print(\"BoW trigram Feature Shape with 7000 features:\", X_bow_trigram.shape)\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow_trigram, df['sentiment_numeric'], test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW using trigrams with 7000 features:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fth_cuda_3_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
