{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (can be skipped if Tasks 1-8 have been run before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import textstat\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'\\W|\\d', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "df['cleaned_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment to numeric values: 1 for positive, 0 for negative\n",
    "df['sentiment_numeric'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre processed dataset to a file for further tasks\n",
    "df.to_csv('IMDB_Dataset_Preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction for Sentiment Classification\n",
    "9- Feature Extraction for Sentiment Classification: Convert the text reviews into numerical representations suitable for machine learning models. First, apply the Bag of Words (BoW) method, which represents the text based on word frequency without considering word order. Next, implement TF-IDF to assign higher importance to less frequent but more meaningful words in the reviews. Finally, explore word embeddings such as Word2Vec, GloVe, or BERT to capture more advanced and contextual word representations, providing richer semantic information for the sentiment classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset_Preprocessed.csv') \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data size\n",
    "print(\"Dataset Size:\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag of Words (BoW) model\n",
    "vectorizer_bow = CountVectorizer(max_features=5000)  # Limit to 5000 most frequent words\n",
    "X_bow = vectorizer_bow.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check BoW features\n",
    "print(\"BoW Feature Shape:\", X_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF model\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check TF-IDF features\n",
    "print(\"TF-IDF Feature Shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize cleaned reviews for Word2Vec model\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all reviews\n",
    "df['tokens'] = df['cleaned_review'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4, epochs=10)\n",
    "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "\n",
    "# Check Word2Vec features\n",
    "print(\"Word2Vec Feature Shape:\", X_word2vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction Using Extracted Features\n",
    "10- Sentiment Prediction Using Extracted Features: Build a sentiment classification model using the features extracted in Task 9. Train the model on the training dataset using features extracted via Bag of Words (BoW), TF-IDF, and word embeddings such as Word2Vec, GloVe, or BERT. After training, evaluate the performance of the model on the test dataset. The goal is to predict whether a review is positive or negative based on these numerical representations. You are required to compare the performance of various classifiers, including Logistic Regression, Support Vector Machines (SVM), Random Forest, and Deep Learning models (LSTM or CNN). Each classifier will be applied to BoW, TF-IDF and word embeddings, and the results should be evaluated using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM related classes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatih\\miniconda3\\envs\\fth_cuda_3_8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# 1. Define Custom Dataset Class\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# 2. Define LSTM Model Class\n",
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 3. Training Loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 4. Create Test Function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).view(-1, 1)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds).flatten().round()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "    return all_targets, all_preds\n",
    "\n",
    "\n",
    "# Define LSTM Training Parameters\n",
    "input_size = 5000  # Input size (BoW features)\n",
    "hidden_size = 128  # Example hidden layer size\n",
    "num_layers = 2 # Number of LSTM layers\n",
    "output_size = 1 # Output size (single scalar value)\n",
    "batch_size = 32 # Define Batch Size\n",
    "num_epochs = 10 # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - BoW with LR, SVM and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - BoW with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_bow Feature Shape:\", X_bow.shape)\n",
    "\n",
    "# Convert to numpy array\n",
    "X_bow = np.asarray(X_bow)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_bow)\n",
    "X_bow = model.transform(X_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['sentiment_numeric'].values, test_size=0.5, random_state=42)\n",
    "print(\"Train Feature Shape:\", X_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Feature Shape:\", X_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)\n",
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "test_dataset = NumpyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Instantiate Model, Define Loss and Optimizer\n",
    "model = LSTMNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Run Training Loops\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Test\n",
    "y_test, y_pre = test_model(model, test_loader)\n",
    "print(\"LSTM with BoW:\")\n",
    "evaluate_model(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - TF-IDF with LR, SVM and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with TF-IDF:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with TF-IDF:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with TF-IDF:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - TF-IDF with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_tfidf Feature Shape:\", X_tfidf.shape)\n",
    "\n",
    "# Convert to numpy array\n",
    "X_tfidf = np.asarray(X_tfidf)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_tfidf)\n",
    "X_tfidf = model.transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment_numeric'].values, test_size=0.5, random_state=42)\n",
    "print(\"Train Feature Shape:\", X_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Feature Shape:\", X_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)\n",
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "test_dataset = NumpyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Instantiate Model, Define Loss and Optimizer\n",
    "model = LSTMNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Run Training Loops\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Test\n",
    "y_test, y_pre = test_model(model, test_loader)\n",
    "print(\"LSTM with TF-IDF:\")\n",
    "evaluate_model(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Word2Vec with LR, SVM and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with word2vec\n",
      "Accuracy: 0.87\n",
      "Precision: 0.87\n",
      "Recall: 0.88\n",
      "F1-Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with word2vec\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with word2vec:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.90\n",
      "Recall: 0.82\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SGDClassifier(loss='hinge')  # 'hinge' loss corresponds to a linear SVM\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with word2vec:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.83\n",
      "Recall: 0.85\n",
      "F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Word2Vec with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_word2vec Feature Shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "input_size = 100  # Input size (Word2Vec features)\n",
    "\n",
    "print(\"X_word2vec Feature Shape:\", X_word2vec.shape)\n",
    "\n",
    "# Convert to numpy array\n",
    "X_word2vec = np.asarray(X_word2vec)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_word2vec)\n",
    "X_word2vec = model.transform(X_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Feature Shape: (25000, 100)\n",
      "Train Labels Shape: (25000,)\n",
      "Test Feature Shape: (25000, 100)\n",
      "Test Labels Shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, df['sentiment_numeric'].values, test_size=0.5, random_state=42)\n",
    "print(\"Train Feature Shape:\", X_train.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Feature Shape:\", X_test.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)\n",
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "test_dataset = NumpyDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0786\n",
      "Epoch [2/10], Loss: 0.1387\n",
      "Epoch [3/10], Loss: 0.0807\n",
      "Epoch [4/10], Loss: 0.0318\n",
      "Epoch [5/10], Loss: 0.0109\n",
      "Epoch [6/10], Loss: 0.0939\n",
      "Epoch [7/10], Loss: 0.0232\n",
      "Epoch [8/10], Loss: 0.0217\n",
      "Epoch [9/10], Loss: 0.0036\n",
      "Epoch [10/10], Loss: 0.0258\n"
     ]
    }
   ],
   "source": [
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Instantiate Model, Define Loss and Optimizer\n",
    "model = LSTMNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Run Training Loops\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM with word2vec:\n",
      "Accuracy: 0.87\n",
      "Precision: 0.85\n",
      "Recall: 0.89\n",
      "F1-Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Run Test\n",
    "y_test, y_pre = test_model(model, test_loader)\n",
    "print(\"LSTM with word2vec:\")\n",
    "evaluate_model(y_test, y_pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fth_cuda_3_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
