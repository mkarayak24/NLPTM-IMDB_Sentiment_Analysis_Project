{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/murat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/murat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/murat/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/murat/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>sentiment_numeric</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>170</td>\n",
       "      <td>1134</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "      <td>1</td>\n",
       "      <td>69.6</td>\n",
       "      <td>72.47</td>\n",
       "      <td>0.829412</td>\n",
       "      <td>82</td>\n",
       "      <td>33</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>['one', 'reviewer', 'mentioned', 'watching', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.9941</td>\n",
       "      <td>0.023881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>90</td>\n",
       "      <td>658</td>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "      <td>1</td>\n",
       "      <td>41.9</td>\n",
       "      <td>44.89</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>['wonderful', 'little', 'production', 'br', 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>0.127604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>87</td>\n",
       "      <td>582</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "      <td>38.4</td>\n",
       "      <td>42.62</td>\n",
       "      <td>0.942529</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9513</td>\n",
       "      <td>0.264732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>70</td>\n",
       "      <td>459</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>33.14</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>['basically', 'there', 'family', 'little', 'bo...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.9061</td>\n",
       "      <td>0.018056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>130</td>\n",
       "      <td>864</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>1</td>\n",
       "      <td>55.2</td>\n",
       "      <td>57.85</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>65</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9871</td>\n",
       "      <td>0.244901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review sentiment  \\\n",
       "0           0  One of the other reviewers has mentioned that ...  positive   \n",
       "1           1  A wonderful little production. <br /><br />The...  positive   \n",
       "2           2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3           3  Basically there's a family where a little boy ...  negative   \n",
       "4           4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "   word_count  char_count                                     cleaned_review  \\\n",
       "0         170        1134  one reviewer mentioned watching oz episode you...   \n",
       "1          90         658  wonderful little production br br filming tech...   \n",
       "2          87         582  thought wonderful way spend time hot summer we...   \n",
       "3          70         459  basically there family little boy jake think t...   \n",
       "4         130         864  petter matteis love time money visually stunni...   \n",
       "\n",
       "   sentiment_numeric  flesch_kincaid_grade  gunning_fog_index  \\\n",
       "0                  1                  69.6              72.47   \n",
       "1                  1                  41.9              44.89   \n",
       "2                  1                  38.4              42.62   \n",
       "3                  0                  31.8              33.14   \n",
       "4                  1                  55.2              57.85   \n",
       "\n",
       "   lexical_diversity  nouns  verbs  adjectives  adverbs  \\\n",
       "0           0.829412     82     33          41        9   \n",
       "1           0.855556     37     19          20       11   \n",
       "2           0.942529     41     18          19        6   \n",
       "3           0.785714     32     13          14        5   \n",
       "4           0.784615     65     23          29        5   \n",
       "\n",
       "                                              tokens  dominant_topic  \\\n",
       "0  ['one', 'reviewer', 'mentioned', 'watching', '...               2   \n",
       "1  ['wonderful', 'little', 'production', 'br', 'b...               0   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...               2   \n",
       "3  ['basically', 'there', 'family', 'little', 'bo...               2   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...               0   \n",
       "\n",
       "  vader_sentiment textblob_sentiment  vader_polarity  textblob_polarity  \n",
       "0        negative           positive         -0.9941           0.023881  \n",
       "1        positive           positive          0.9571           0.127604  \n",
       "2        positive           positive          0.9513           0.264732  \n",
       "3        negative           positive         -0.9061           0.018056  \n",
       "4        positive           positive          0.9871           0.244901  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset_Preprocessed.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Check data size\n",
    "print(\"Dataset Size:\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Feature Extraction for Sentiment Classification: Convert the text reviews into numerical representations suitable for\n",
    "machine learning models. First, apply the Bag of Words (BoW) method, which represents the text based on word frequency\n",
    "without considering word order. Next, implement TF-IDF to assign higher importance to less frequent but more meaningful words in the reviews. Finally, explore word embeddings such as Word2Vec, GloVe, or BERT to capture more advanced and\n",
    "contextual word representations, providing richer semantic information for the sentiment classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create Bag of Words (BoW) model\n",
    "vectorizer_bow = CountVectorizer(max_features=5000)  # Limit to 5000 most frequent words\n",
    "X_bow = vectorizer_bow.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check BoW features\n",
    "print(\"BoW Feature Shape:\", X_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF model\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check TF-IDF features\n",
    "print(\"TF-IDF Feature Shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4)\n",
    "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "\n",
    "# Check Word2Vec features\n",
    "print(\"Word2Vec Feature Shape:\", X_word2vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Feature Shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional embeddings\n",
    "\n",
    "# Convert reviews to GloVe vectors\n",
    "def get_glove_embeddings(review):\n",
    "    words = review.split()\n",
    "    return np.mean([glove_model[word] for word in words if word in glove_model] or [np.zeros(100)], axis=0)\n",
    "\n",
    "X_glove = np.array([get_glove_embeddings(review) for review in df['cleaned_review']])\n",
    "\n",
    "# Check GloVe features\n",
    "print(\"GloVe Feature Shape:\", X_glove.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- Sentiment Prediction Using Extracted Features: Build a sentiment classification model using the features extracted in\n",
    "Task 9. Train the model on the training dataset using features extracted via Bag of Words (BoW), TF-IDF, and word\n",
    "embeddings such as Word2Vec, GloVe, or BERT. After training, evaluate the performance of the model on the test dataset.\n",
    "The goal is to predict whether a review is positive or negative based on these numerical representations. You are required to\n",
    "compare the performance of various classifiers, including Logistic Regression, Support Vector Machines (SVM), Random\n",
    "Forest, and Deep Learning models (LSTM or CNN). Each classifier will be applied to BoW, TF-IDF and word embeddings,\n",
    "and the results should be evaluated using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    recall = recall_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    f1 = f1_score(true_labels, predicted_labels, pos_label='positive')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['sentiment'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with BoW:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.86\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with BoW:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.85\n",
      "Recall: 0.84\n",
      "F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 01:19:18.651315: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node compile_loss/binary_crossentropy/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/var/folders/xz/tpjwm4l52hjdcg11dtxxgyxr0000gn/T/ipykernel_50666/1377160367.py\", line 8, in <module>\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/trainer.py\", line 398, in _compute_loss\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/trainer.py\", line 366, in compute_loss\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/compile_utils.py\", line 618, in __call__\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/compile_utils.py\", line 659, in call\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/losses/loss.py\", line 56, in __call__\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/tree/tree_api.py\", line 148, in map_structure\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/tree/optree_impl.py\", line 79, in map_structure\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/optree/ops.py\", line 747, in tree_map\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/losses/loss.py\", line 57, in <lambda>\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/ops/core.py\", line 917, in convert_to_tensor\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py\", line 132, in convert_to_tensor\n\nCast string to float is not supported\n\t [[{{node compile_loss/binary_crossentropy/Cast}}]] [Op:__inference_one_step_on_iterator_9117]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m lstm_model_bow\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m lstm_model_bow\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m lstm_model_bow\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM with BoW:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m evaluate_model(y_test, lstm_model_bow\u001b[38;5;241m.\u001b[39mpredict(X_test))\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node compile_loss/binary_crossentropy/Cast defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 639, in run_forever\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/asyncio/base_events.py\", line 1985, in _run_once\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/var/folders/xz/tpjwm4l52hjdcg11dtxxgyxr0000gn/T/ipykernel_50666/1377160367.py\", line 8, in <module>\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 54, in train_step\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/trainer.py\", line 398, in _compute_loss\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/trainer.py\", line 366, in compute_loss\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/compile_utils.py\", line 618, in __call__\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/trainers/compile_utils.py\", line 659, in call\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/losses/loss.py\", line 56, in __call__\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/tree/tree_api.py\", line 148, in map_structure\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/tree/optree_impl.py\", line 79, in map_structure\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/optree/ops.py\", line 747, in tree_map\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/losses/loss.py\", line 57, in <lambda>\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/ops/core.py\", line 917, in convert_to_tensor\n\n  File \"/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/core.py\", line 132, in convert_to_tensor\n\nCast string to float is not supported\n\t [[{{node compile_loss/binary_crossentropy/Cast}}]] [Op:__inference_one_step_on_iterator_9117]"
     ]
    }
   ],
   "source": [
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "print(\"LSTM with BoW:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model with tf-idf:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.87\n",
      "Recall: 0.90\n",
      "F1-Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with tf-idf:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with tf-idf:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Precision: 0.87\n",
      "Recall: 0.90\n",
      "F1-Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with tf-idf:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "print(\"LSTM with tf-idf:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, df['sentiment'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.85\n",
      "Recall: 0.86\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "print(\"LSTM with word2vec:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_glove, df['sentiment'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_glove = LogisticRegression(max_iter=1000)\n",
    "clf_glove.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with glove\")\n",
    "evaluate_model(y_test, clf_glove.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_glove = SVC(kernel='linear')\n",
    "svm_glove.fit(X_train, y_train)\n",
    "print(\"SVM with glove:\")\n",
    "evaluate_model(y_test, svm_glove.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.76\n",
      "Recall: 0.77\n",
      "F1-Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_glove = RandomForestClassifier(n_estimators=100)\n",
    "rf_glove.fit(X_train, y_train)\n",
    "print(\"Random Forest with glove:\")\n",
    "evaluate_model(y_test, rf_glove.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "lstm_model_glove = Sequential()\n",
    "lstm_model_glove.add(Embedding(input_dim=word2vec_model.wv.vectors.shape[0], output_dim=100, input_length=100)) \n",
    "lstm_model_glove.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_glove.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_glove.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "print(\"LSTM with glove:\")\n",
    "evaluate_model(y_test, lstm_model_glove.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
