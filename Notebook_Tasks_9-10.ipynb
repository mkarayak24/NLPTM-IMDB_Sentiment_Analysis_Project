{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>sentiment_numeric</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>166</td>\n",
       "      <td>1116</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.98</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>78</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>['one', 'reviewer', 'mentioned', 'watching', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.9941</td>\n",
       "      <td>0.023881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>84</td>\n",
       "      <td>640</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.8</td>\n",
       "      <td>43.12</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>['wonderful', 'little', 'production', 'filming...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>0.127604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>85</td>\n",
       "      <td>572</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "      <td>37.6</td>\n",
       "      <td>41.53</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.278571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>67</td>\n",
       "      <td>443</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>32.17</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>['basically', 'there', 'family', 'little', 'bo...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.9061</td>\n",
       "      <td>0.018056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>125</td>\n",
       "      <td>843</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>1</td>\n",
       "      <td>53.2</td>\n",
       "      <td>55.44</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>61</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>0.239534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review sentiment  \\\n",
       "0           0  One of the other reviewers has mentioned that ...  positive   \n",
       "1           1  A wonderful little production. <br /><br />The...  positive   \n",
       "2           2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3           3  Basically there's a family where a little boy ...  negative   \n",
       "4           4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "   word_count  char_count                                     cleaned_review  \\\n",
       "0         166        1116  one reviewer mentioned watching oz episode you...   \n",
       "1          84         640  wonderful little production filming technique ...   \n",
       "2          85         572  thought wonderful way spend time hot summer we...   \n",
       "3          67         443  basically there family little boy jake think t...   \n",
       "4         125         843  petter matteis love time money visually stunni...   \n",
       "\n",
       "   sentiment_numeric  flesch_kincaid_grade  gunning_fog_index  \\\n",
       "0                  1                  68.0              70.98   \n",
       "1                  1                  40.8              43.12   \n",
       "2                  1                  37.6              41.53   \n",
       "3                  0                  30.6              32.17   \n",
       "4                  1                  53.2              55.44   \n",
       "\n",
       "   lexical_diversity  nouns  verbs  adjectives  adverbs  \\\n",
       "0           0.825301     78     33          40       10   \n",
       "1           0.904762     33     18          20       11   \n",
       "2           0.952941     39     19          18        6   \n",
       "3           0.791045     32     13          12        5   \n",
       "4           0.800000     61     23          29        5   \n",
       "\n",
       "                                              tokens  dominant_topic  \\\n",
       "0  ['one', 'reviewer', 'mentioned', 'watching', '...               0   \n",
       "1  ['wonderful', 'little', 'production', 'filming...               1   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...               2   \n",
       "3  ['basically', 'there', 'family', 'little', 'bo...               2   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...               1   \n",
       "\n",
       "  vader_sentiment textblob_sentiment  vader_polarity  textblob_polarity  \n",
       "0        negative           positive         -0.9941           0.023881  \n",
       "1        positive           positive          0.9571           0.127604  \n",
       "2        positive           positive          0.9688           0.278571  \n",
       "3        negative           positive         -0.9061           0.018056  \n",
       "4        positive           positive          0.9887           0.239534  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset_Preprocessed.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Check data size\n",
    "print(\"Dataset Size:\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Feature Extraction for Sentiment Classification: Convert the text reviews into numerical representations suitable for\n",
    "machine learning models. First, apply the Bag of Words (BoW) method, which represents the text based on word frequency\n",
    "without considering word order. Next, implement TF-IDF to assign higher importance to less frequent but more meaningful words in the reviews. Finally, explore word embeddings such as Word2Vec, GloVe, or BERT to capture more advanced and\n",
    "contextual word representations, providing richer semantic information for the sentiment classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create Bag of Words (BoW) model\n",
    "vectorizer_bow = CountVectorizer(max_features=5000)  # Limit to 5000 most frequent words\n",
    "X_bow = vectorizer_bow.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check BoW features\n",
    "print(\"BoW Feature Shape:\", X_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF model\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check TF-IDF features\n",
    "print(\"TF-IDF Feature Shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4)\n",
    "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "\n",
    "# Check Word2Vec features\n",
    "print(\"Word2Vec Feature Shape:\", X_word2vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Feature Shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-100\")  # 100-dimensional embeddings\n",
    "\n",
    "# Convert reviews to GloVe vectors\n",
    "def get_glove_embeddings(review):\n",
    "    words = review.split()\n",
    "    return np.mean([glove_model[word] for word in words if word in glove_model] or [np.zeros(100)], axis=0)\n",
    "\n",
    "X_glove = np.array([get_glove_embeddings(review) for review in df['cleaned_review']])\n",
    "\n",
    "# Check GloVe features\n",
    "print(\"GloVe Feature Shape:\", X_glove.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- Sentiment Prediction Using Extracted Features: Build a sentiment classification model using the features extracted in\n",
    "Task 9. Train the model on the training dataset using features extracted via Bag of Words (BoW), TF-IDF, and word\n",
    "embeddings such as Word2Vec, GloVe, or BERT. After training, evaluate the performance of the model on the test dataset.\n",
    "The goal is to predict whether a review is positive or negative based on these numerical representations. You are required to\n",
    "compare the performance of various classifiers, including Logistic Regression, Support Vector Machines (SVM), Random\n",
    "Forest, and Deep Learning models (LSTM or CNN). Each classifier will be applied to BoW, TF-IDF and word embeddings,\n",
    "and the results should be evaluated using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print evaluation metrics\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with BoW:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.86\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with BoW:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.85\n",
      "Recall: 0.84\n",
      "F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/murat/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m lstm_model_bow\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m lstm_model_bow\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m lstm_model_bow\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM with BoW:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m evaluate_model(y_test, lstm_model_bow\u001b[38;5;241m.\u001b[39mpredict(X_test))\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    920\u001b[0m       filtered_flat_args,\n\u001b[1;32m    921\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[1;32m    922\u001b[0m   )\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM model:\n",
    "lstm_model_bow = Sequential()\n",
    "lstm_model_bow.add(Embedding(input_dim=5000, output_dim=128, input_length=X_train.shape[1])) \n",
    "lstm_model_bow.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_bow.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_bow.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_bow.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "print(\"LSTM with BoW:\")\n",
    "evaluate_model(y_test, lstm_model_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model with tf-idf:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.87\n",
      "Recall: 0.90\n",
      "F1-Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with tf-idf:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with tf-idf:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n",
      "Precision: 0.87\n",
      "Recall: 0.90\n",
      "F1-Score: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with tf-idf:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "lstm_model_tfidf = Sequential()\n",
    "lstm_model_tfidf.add(Embedding(input_dim=5000, output_dim=100, input_length=100)) \n",
    "lstm_model_tfidf.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_tfidf.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_tfidf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_tfidf.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "print(\"LSTM with tf-idf:\")\n",
    "evaluate_model(y_test, lstm_model_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.85\n",
      "Recall: 0.86\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "lstm_model_word2vec = Sequential()\n",
    "lstm_model_word2vec.add(Embedding(input_dim=100, output_dim=100, input_length=100)) \n",
    "lstm_model_word2vec.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_word2vec.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_word2vec.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_word2vec.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "print(\"LSTM with word2vec:\")\n",
    "evaluate_model(y_test, lstm_model_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_glove, df['sentiment_numeric'], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_glove = LogisticRegression(max_iter=1000)\n",
    "clf_glove.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with glove\")\n",
    "evaluate_model(y_test, clf_glove.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_glove = SVC(kernel='linear')\n",
    "svm_glove.fit(X_train, y_train)\n",
    "print(\"SVM with glove:\")\n",
    "evaluate_model(y_test, svm_glove.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Precision: 0.76\n",
      "Recall: 0.77\n",
      "F1-Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_glove = RandomForestClassifier(n_estimators=100)\n",
    "rf_glove.fit(X_train, y_train)\n",
    "print(\"Random Forest with glove:\")\n",
    "evaluate_model(y_test, rf_glove.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model:\n",
    "lstm_model_glove = Sequential()\n",
    "lstm_model_glove.add(Embedding(input_dim=100, output_dim=100, input_length=100)) \n",
    "lstm_model_glove.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm_model_glove.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model_glove.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model_glove.fit(X_train, y_train, epochs=1, batch_size=32)\n",
    "\n",
    "print(\"LSTM with glove:\")\n",
    "evaluate_model(y_test, lstm_model_glove.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
