{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>sentiment_numeric</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog_index</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>nouns</th>\n",
       "      <th>verbs</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adverbs</th>\n",
       "      <th>tokens</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>vader_sentiment</th>\n",
       "      <th>textblob_sentiment</th>\n",
       "      <th>vader_polarity</th>\n",
       "      <th>textblob_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>166</td>\n",
       "      <td>1116</td>\n",
       "      <td>one reviewer mentioned watching oz episode you...</td>\n",
       "      <td>1</td>\n",
       "      <td>68.0</td>\n",
       "      <td>70.98</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>78</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>['one', 'reviewer', 'mentioned', 'watching', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.9941</td>\n",
       "      <td>0.023881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>84</td>\n",
       "      <td>640</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "      <td>40.8</td>\n",
       "      <td>43.12</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>['wonderful', 'little', 'production', 'filming...</td>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9571</td>\n",
       "      <td>0.127604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>85</td>\n",
       "      <td>572</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "      <td>37.6</td>\n",
       "      <td>41.53</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>['thought', 'wonderful', 'way', 'spend', 'time...</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.278571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>67</td>\n",
       "      <td>443</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "      <td>0</td>\n",
       "      <td>30.6</td>\n",
       "      <td>32.17</td>\n",
       "      <td>0.791045</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>['basically', 'there', 'family', 'little', 'bo...</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>positive</td>\n",
       "      <td>-0.9061</td>\n",
       "      <td>0.018056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>125</td>\n",
       "      <td>843</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>1</td>\n",
       "      <td>53.2</td>\n",
       "      <td>55.44</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>61</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>['petter', 'matteis', 'love', 'time', 'money',...</td>\n",
       "      <td>7</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.9887</td>\n",
       "      <td>0.239534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                             review sentiment  \\\n",
       "0           0  One of the other reviewers has mentioned that ...  positive   \n",
       "1           1  A wonderful little production. <br /><br />The...  positive   \n",
       "2           2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3           3  Basically there's a family where a little boy ...  negative   \n",
       "4           4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "   word_count  char_count                                     cleaned_review  \\\n",
       "0         166        1116  one reviewer mentioned watching oz episode you...   \n",
       "1          84         640  wonderful little production filming technique ...   \n",
       "2          85         572  thought wonderful way spend time hot summer we...   \n",
       "3          67         443  basically there family little boy jake think t...   \n",
       "4         125         843  petter matteis love time money visually stunni...   \n",
       "\n",
       "   sentiment_numeric  flesch_kincaid_grade  gunning_fog_index  \\\n",
       "0                  1                  68.0              70.98   \n",
       "1                  1                  40.8              43.12   \n",
       "2                  1                  37.6              41.53   \n",
       "3                  0                  30.6              32.17   \n",
       "4                  1                  53.2              55.44   \n",
       "\n",
       "   lexical_diversity  nouns  verbs  adjectives  adverbs  \\\n",
       "0           0.825301     78     33          40       10   \n",
       "1           0.904762     33     18          20       11   \n",
       "2           0.952941     39     19          18        6   \n",
       "3           0.791045     32     13          12        5   \n",
       "4           0.800000     61     23          29        5   \n",
       "\n",
       "                                              tokens  dominant_topic  \\\n",
       "0  ['one', 'reviewer', 'mentioned', 'watching', '...               2   \n",
       "1  ['wonderful', 'little', 'production', 'filming...               6   \n",
       "2  ['thought', 'wonderful', 'way', 'spend', 'time...               3   \n",
       "3  ['basically', 'there', 'family', 'little', 'bo...               3   \n",
       "4  ['petter', 'matteis', 'love', 'time', 'money',...               7   \n",
       "\n",
       "  vader_sentiment textblob_sentiment  vader_polarity  textblob_polarity  \n",
       "0        negative           positive         -0.9941           0.023881  \n",
       "1        positive           positive          0.9571           0.127604  \n",
       "2        positive           positive          0.9688           0.278571  \n",
       "3        negative           positive         -0.9061           0.018056  \n",
       "4        positive           positive          0.9887           0.239534  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('IMDB_Dataset_Preprocessed.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Check data size\n",
    "print(\"Dataset Size:\")\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9- Feature Extraction for Sentiment Classification: Convert the text reviews into numerical representations suitable for\n",
    "machine learning models. First, apply the Bag of Words (BoW) method, which represents the text based on word frequency\n",
    "without considering word order. Next, implement TF-IDF to assign higher importance to less frequent but more meaningful words in the reviews. Finally, explore word embeddings such as Word2Vec, GloVe, or BERT to capture more advanced and\n",
    "contextual word representations, providing richer semantic information for the sentiment classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Feature Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create Bag of Words (BoW) model\n",
    "vectorizer_bow = CountVectorizer(max_features=5000)  # Limit to 5000 most frequent words\n",
    "X_bow = vectorizer_bow.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check BoW features\n",
    "print(\"BoW Feature Shape:\", X_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_bow)\n",
    "X_bow = model.transform(X_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Shape: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF model\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Check TF-IDF features\n",
    "print(\"TF-IDF Feature Shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_tfidf)\n",
    "X_tfidf = model.transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Feature Shape: (50000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to all reviews\n",
    "df['tokens'] = df['cleaned_review'].apply(tokenize_text)\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=5, workers=4)\n",
    "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in df['tokens']])\n",
    "\n",
    "# Check Word2Vec features\n",
    "print(\"Word2Vec Feature Shape:\", X_word2vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "model = scaler.fit(X_word2vec)\n",
    "X_word2vec = model.transform(X_word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10- Sentiment Prediction Using Extracted Features: Build a sentiment classification model using the features extracted in\n",
    "Task 9. Train the model on the training dataset using features extracted via Bag of Words (BoW), TF-IDF, and word\n",
    "embeddings such as Word2Vec, GloVe, or BERT. After training, evaluate the performance of the model on the test dataset.\n",
    "The goal is to predict whether a review is positive or negative based on these numerical representations. You are required to\n",
    "compare the performance of various classifiers, including Logistic Regression, Support Vector Machines (SVM), Random\n",
    "Forest, and Deep Learning models (LSTM or CNN). Each classifier will be applied to BoW, TF-IDF and word embeddings,\n",
    "and the results should be evaluated using metrics such as accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Custom Dataset Class\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# 2. Define LSTM Model Class\n",
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x.unsqueeze(1), (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# 3. Training Loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "\n",
    "# 4. Create Test Function\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).view(-1, 1)\n",
    "            outputs = model(inputs)\n",
    "            all_preds.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds).flatten().round()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "    return all_targets, all_preds\n",
    "\n",
    "# 5. Create Evaluation Function\n",
    "def evaluate_model(true_labels, predicted_labels):\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Parameters\n",
    "hidden_size = 128  # Example hidden layer size\n",
    "num_layers = 2 # Number of LSTM layers\n",
    "output_size = 1 # Output size (single scalar value)\n",
    "batch_size = 256 # Define Batch Size\n",
    "num_epochs = 10 # Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, df['sentiment_numeric'].values, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with BoW:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.82\n",
      "Recall: 0.82\n",
      "F1-Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "print(\"Logistic Regression with BoW:\")\n",
    "evaluate_model(y_test, clf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with BoW:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.82\n",
      "Recall: 0.82\n",
      "F1-Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_bow = SVC(kernel='linear')\n",
    "svm_bow.fit(X_train, y_train)\n",
    "print(\"SVM with BoW:\")\n",
    "evaluate_model(y_test, svm_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with BoW:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.85\n",
      "Recall: 0.84\n",
      "F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_bow = RandomForestClassifier(n_estimators=100)\n",
    "rf_bow.fit(X_train, y_train)\n",
    "print(\"Random Forest with BoW:\")\n",
    "evaluate_model(y_test, rf_bow.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.09935626\n",
      "Epoch [2/10], Loss: 0.01869978\n",
      "Epoch [3/10], Loss: 0.00050604\n",
      "Epoch [4/10], Loss: 0.00016974\n",
      "Epoch [5/10], Loss: 0.00011066\n",
      "Epoch [6/10], Loss: 0.00010869\n",
      "Epoch [7/10], Loss: 0.00004452\n",
      "Epoch [8/10], Loss: 0.00003513\n",
      "Epoch [9/10], Loss: 0.00002754\n",
      "Epoch [10/10], Loss: 0.00002235\n",
      "LSTM with BoW:\n",
      "Accuracy: 0.87\n",
      "Precision: 0.86\n",
      "Recall: 0.87\n",
      "F1-Score: 0.87\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "# Prepare Dataset\n",
    "input_size = 5000  # Number of features\n",
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "test_dataset = NumpyDataset(X_test, y_test)\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Instantiate Model, Define Loss and Optimizer\n",
    "model = LSTMNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Run Training Loops\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "# Run Test\n",
    "y_test, y_pre = test_model(model, test_loader)\n",
    "print(\"LSTM with BoW:\")\n",
    "evaluate_model(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['sentiment_numeric'].values, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model with tf-idf:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.82\n",
      "Recall: 0.82\n",
      "F1-Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with tf-idf:\")\n",
    "evaluate_model(y_test, clf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with tf-idf:\n",
      "Accuracy: 0.81\n",
      "Precision: 0.81\n",
      "Recall: 0.81\n",
      "F1-Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_tfidf = SVC(kernel='linear')\n",
    "svm_tfidf.fit(X_train, y_train)\n",
    "print(\"SVM with tf-idf:\")\n",
    "evaluate_model(y_test, svm_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with tf-idf:\n",
      "Accuracy: 0.84\n",
      "Precision: 0.84\n",
      "Recall: 0.84\n",
      "F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_tfidf = RandomForestClassifier(n_estimators=100)\n",
    "rf_tfidf.fit(X_train, y_train)\n",
    "print(\"Random Forest with tf-idf:\")\n",
    "evaluate_model(y_test, rf_tfidf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.09836783\n",
      "Epoch [2/10], Loss: 0.00815309\n",
      "Epoch [3/10], Loss: 0.00364960\n",
      "Epoch [4/10], Loss: 0.00011586\n",
      "Epoch [5/10], Loss: 0.00008695\n",
      "Epoch [6/10], Loss: 0.00005566\n",
      "Epoch [7/10], Loss: 0.00004445\n",
      "Epoch [8/10], Loss: 0.00003449\n",
      "Epoch [9/10], Loss: 0.00002536\n",
      "Epoch [10/10], Loss: 0.00002077\n",
      "LSTM with tf-idf:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.85\n",
      "Recall: 0.88\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "# Prepare Dataset\n",
    "input_size = 5000  # Number of features\n",
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "test_dataset = NumpyDataset(X_test, y_test)\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Instantiate Model, Define Loss and Optimizer\n",
    "model = LSTMNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Run Training Loops\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "# Run Test\n",
    "y_test, y_pre = test_model(model, test_loader)\n",
    "print(\"LSTM with tf-idf:\")\n",
    "evaluate_model(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_word2vec, df['sentiment_numeric'].values, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model with word2vec\n",
      "Accuracy: 0.86\n",
      "Precision: 0.85\n",
      "Recall: 0.87\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression model\n",
    "clf_word2vec = LogisticRegression(max_iter=1000)\n",
    "clf_word2vec.fit(X_train, y_train)\n",
    "print(\"Logistic Regression model with word2vec\")\n",
    "evaluate_model(y_test, clf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with word2vec:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.85\n",
      "Recall: 0.87\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Train a Support Vector Machine (SVM)\n",
    "svm_word2vec = SVC(kernel='linear')\n",
    "svm_word2vec.fit(X_train, y_train)\n",
    "print(\"SVM with word2vec:\")\n",
    "evaluate_model(y_test, svm_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with word2vec:\n",
      "Accuracy: 0.83\n",
      "Precision: 0.82\n",
      "Recall: 0.85\n",
      "F1-Score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Train a RF Classifier\n",
    "rf_word2vec = RandomForestClassifier(n_estimators=100)\n",
    "rf_word2vec.fit(X_train, y_train)\n",
    "print(\"Random Forest with word2vec:\")\n",
    "evaluate_model(y_test, rf_word2vec.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.10251184\n",
      "Epoch [2/10], Loss: 0.10751328\n",
      "Epoch [3/10], Loss: 0.10290660\n",
      "Epoch [4/10], Loss: 0.08522266\n",
      "Epoch [5/10], Loss: 0.07654339\n",
      "Epoch [6/10], Loss: 0.06789526\n",
      "Epoch [7/10], Loss: 0.08451746\n",
      "Epoch [8/10], Loss: 0.07823770\n",
      "Epoch [9/10], Loss: 0.06051233\n",
      "Epoch [10/10], Loss: 0.06286611\n",
      "LSTM with word2vec:\n",
      "Accuracy: 0.86\n",
      "Precision: 0.86\n",
      "Recall: 0.87\n",
      "F1-Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "# Prepare Dataset\n",
    "input_size = 100  # Number of features\n",
    "train_dataset = NumpyDataset(X_train, y_train)\n",
    "test_dataset = NumpyDataset(X_test, y_test)\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Instantiate Model, Define Loss and Optimizer\n",
    "model = LSTMNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Run Training Loops\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "# Run Test\n",
    "y_test, y_pre = test_model(model, test_loader)\n",
    "print(\"LSTM with word2vec:\")\n",
    "evaluate_model(y_test, y_pre)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
